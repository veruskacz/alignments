# from sys import stderr
import networkx as nx
from os import makedirs
import time
import datetime
from os.path import join, isdir
import Alignments.Utility as Ut
import Alignments.Query as Qry
from Alignments.Utility import get_uri_local_name_plus as local_name
from Alignments.UserActivities.Clustering import links_clustering
from Alignments.UserActivities.Plots import metric
import cStringIO as Buffer


# CLUSTER ID
# CLUSTER SIZE
# MACHINE EVAL
# HUMAN EVAL
# RESOURCES


# **************************************************************************************
# THIS FUNCTION BUILDS CONTEXTUAL INFORMATION ON A [SET OF PROVIDED RESOURCES] BASED ON
# [DATASETS] AND [MANDATORY OR OPTIONAL PROPERTIES] OF INTEREST
# **************************************************************************************
def investigate_resources(data, resources):

    # DATASET AND THE PROPERTIES OF INTEREST. EACH PROPERTY CAN BE PROVIDED WITH AN ALTERNATIVE NAME
    # data = {
    #     'dataset-1': {
    #         'mandatory': [('property-1', 'name'), ('property-10', ""), ('property-15', '')],
    #         'optional': [('property-00', 'country'), ]
    #     },
    #     'dataset-2': {
    #         'mandatory': [('property-2', 'name'), ('property-23', ""), ('property-30', "")],
    #         'optional': []
    #     },
    #     'dataset-3': {
    #         'mandatory': [('property-3', 'name'), ('property-36', ""), ('property-33', "")],
    #         'optional': []
    #     }
    # }

    # THE LIST OF RESOURCES TO INVESTIGATE
    # resource = ['resource-1', 'resource-2', 'resource-3']

    # QUERY TEMPLE EXAMPLE
    """
    SELECT *
    {
        # LIST OR RESOURCES TO INVESTIGATE
        VALUES ?resource { resource-1 resource-2 resource-3 }

        {
            GRAPH <dataset-2>
            {
                BIND( <dataset-2> as ?dataset)
                ?resource <property-2> ?name .
                ?resource <property-23> ?dataset-2_property-23 .
                ?resource <property-30> ?dataset-2_property-30 .
            }
        }	UNION

        {
            GRAPH <dataset-3>
            {
                BIND( <dataset-3> as ?dataset)
                ?resource <property-3> ?name .
                ?resource <property-36> ?dataset-3_property-36 .
                ?resource <property-33> ?dataset-3_property-33 .
            }
        }	UNION

        {
            GRAPH <dataset-1>
            {
                BIND( <dataset-1> as ?dataset)
                ?resource <property-1> ?name .
                ?resource <property-10> ?dataset-1_property-10 .
                ?resource <property-15> ?dataset-1_property-15 .
                OPTIONAL { ?resource <property-00> ?country . }
            }
        }
    }
    """

    count_union = 0
    sub_query = ""

    template_1 = """\t{}
        {{
            GRAPH {}
            {{ \n\t\t\t\t{}\n{}{}
            }}
        }}"""

    template_2 = """
    SELECT DISTINCT *
    {{
        # LIST OR RESOURCES TO INVESTIGATE
        VALUES ?resource {{ {} }}
        {}
    }} ORDER BY ?dataset ?resource

    """

    # CONVERTING THE LIST INTO A SPACE SEPARATED LIST
    resource_enumeration = " ".join(Ut.to_nt_format(item) for item in resources)

    for dataset, dictionary in data.items():

        mandatory = dictionary['mandatory']
        optional = dictionary['optional']

        # GENERATE THE SUB-QUERY FOR MANDATORY PROPERTIES
        sub_mandatory = "\n".join(
            "\t\t\t\t?resource {0} ?{1} .".format(
                Ut.to_nt_format(uri), alternative if len(alternative) > 0 else
                "{}_{}".format(local_name(dataset), local_name(uri))) for uri, alternative in mandatory)

        # GENERATE THE SUB-QUERY FOR OPTIONAL PROPERTIES
        if len(optional) > 0:
            sub_optional = "\n".join(
                "\n\t\t\t\tOPTIONAL {{ ?resource {0} ?{1} . }}".format(
                    Ut.to_nt_format(uri), alternative if len(alternative) > 0 else
                    "{}_{}".format(local_name(dataset), local_name(uri))) for uri, alternative in optional)
        else:
            sub_optional = ""

        # BIND THE DATASET TO HAVE IT IN THE SELECT
        bind = "BIND( {} as ?dataset)".format(Ut.to_nt_format(dataset))

        # ACCUMULATE THE SUB-QUERIES
        sub_query += template_1.format(
            "UNION\n" if count_union > 0 else "", Ut.to_nt_format(dataset), bind, sub_mandatory, sub_optional)
        count_union += 1

    # THE FINAL QUERY
    query = template_2.format(resource_enumeration, sub_query)
    # print query
    # Qry.display_result(query, is_activated=True)

    response = Qry.sparql_xml_to_matrix(query)

    if response is None:
        return None
    else:
        return response['result']


# **************************************************************************************
# THIS FUNCTION CONVERTS THE CONTEXTUAL INFORMATION GENERATED BY THE FUNCTION:
# investigate_resources(data, resources) INTO PLAIN LINES SEPARATED BY |
# **************************************************************************************
def get_context(matrix, separator_size=40):

    count = 0
    writer = Buffer.StringIO()
    writer.write("\n")
    format_template = "{{:{}}}".format(separator_size)
    # print format_template

    if matrix is not None:
        for record in matrix:
            count += 1
            record_line = " | ".join(
                format_template.format("") if item is None or len(item) == 0
                else format_template.format(local_name(item.upper())) for item in record)
            writer.write("{}\n".format(record_line))

    else:
        print "THE MATRIX IS EMPTY"

    return writer.getvalue()


# **************************************************************************************
# THIS FUNCTION CONVERTS THE CONTEXTUAL INFORMATION GENERATED BY THE FUNCTION:
# investigate_resources(data, resources) INTO PLAIN LINES LINES SEPARATED BY |
# BUT INSERTS THE LINES INTO A BIGGER FRAME FOR VALIDATION INVESTIGATION.
# THE DATA IS FINALLY WRITTEN TO FILE
# **************************************************************************************
def write_record(count_record, size, record_format, matrix, writer, cluster_id="",
                 separator_size=40, machine_decision="", has_cycle='no', node_in_cycle=None):

    count = 0
    format_template = "{{:{}}}".format(separator_size)
    # print >> stderr, count_record, '\r',
    print "{:<5}".format(count_record),

    if matrix is not None:
        for record in matrix:
            count += 1

            # print record_line
            if count == 1:
                record_line = " | ".join(
                    format_template.format("") if item is None or len(item) == 0
                    else format_template.format(local_name(item.upper())) for item in record)
                writer.write(record_format.format(
                    count_record, cluster_id, size, "-{}-".format(machine_decision),
                    "- -", has_cycle, "", "", record_line))
            else:

                if node_in_cycle is not None:
                    # print local_name(record[0])
                    node = "-X-" if Ut.to_nt_format(record[0]) in node_in_cycle else "- -"
                else:
                    node = "- -"

                record_line = " | ".join(
                    format_template.format("") if item is None or len(item) == 0
                    else format_template.format(local_name(item)) for item in record)
                writer.write(record_format.format("", "", "", "", "", "", "- -", node, record_line))

    else:
        print "THE MATRIX IS EMPTY"


# **************************************************************************************
#  THIS FUNCTION COMBINES
#   - investigate_resources(data, resources)
#   - write_record( size, record_format, matrix, writer, cluster_id="",
#                   separator_size=40, machine_decision="", has_cycle='no')
#
# TO GENERATE AN EVALUATION SHEET BASED ON THE FOLLOWING RETRIEVED/GENERATED INFORMATION:
#   "CLUSTER-ID", "CLUSTER-SIZE", "MACHINE-EVAL", "HUMAN-EVAL",
#   "HAS-CYCLE", "NOT GOOD", "CYCLE", "RESOURCES"
# **************************************************************************************
def generate_sheet(data, directory, graph, serialisation_dir, related_alignment=None, separator_size=40, size=None):

    start = time.time()
    count = 0
    count_c = 0
    count_b = 0
    count_nc = 0
    cycles = None
    # extended = None
    header_separator_size = 23

    # FILE DATE
    # date = datetime.date.isoformat(datetime.date.today()).replace('-', '')
    date = (str(datetime.datetime.utcnow())[:-7]).replace('-', '').replace(':', '').replace(' ', '-')
    hashed = Ut.hash_it(graph)
    directory = join(directory, hashed)

    if isdir(directory) is False:
        makedirs(directory)

    # THE WRITER
    writer_cycle = open(join(directory, "EvalSheet_{}_{}_cycle.txt".format(hashed, date)), 'wb')
    writer_no_cycle = open(join(directory, "EvalSheet_{}_{}_noCycle.txt".format(hashed, date)), 'wb')
    writer_2 = open(join(directory, "EvalSheet_{}_{}_biggerThan_{}.txt".format(hashed, date, size)), 'wb')

    # RECORD FORMAT
    record_format = "{{:<7}}{{:<{0}}}{{:<14}}{{:<{0}}}{{:<12}}{{:<12}}{{:<10}}{{:<7}}{{:<{0}}}\n".format(
        header_separator_size)
    # print record_format

    # RECORD HEADER
    header = record_format.format(
        "COUNT", "CLUSTER-ID", "CLUSTER-SIZE", "MACHINE-EVAL",
        "HUMAN-EVAL", "HAS-CYCLE", "NOT GOOD", "CYCLE", "RESOURCES")

    # WRITING THE FILE HEADER
    writer_cycle.write(header)
    writer_no_cycle.write(header)
    writer_2.write(header)

    # **************************************************************************************
    # 1. GENERATING / EXTRACTING CLUSTERS, EXTENDED CLUSTERS AND LIST OF CLUSTERS IN A CYCLE
    # **************************************************************************************
    # 1.1 IF THE RELATED ALIGNMENT IS NOT PROVIDED, ONLY THE CLUSTERS DICTIONARY IS RETURNED
    if related_alignment is None:
        clusters = links_clustering(graph=graph, serialisation_dir=serialisation_dir)
        cycle_paths = None

    # 1.2 IF THE RELATED ALIGNMENT IS PROVIDED, THEN THE EXTENDED CLUSTERS AND CLUSTERS IN A CYCLE
    # IS COMPUTED IF NT SERIALISED OR READ FROM FILE IF SERIALISED
    else:
        results = links_clustering(
            graph=graph, serialisation_dir=serialisation_dir, related_linkset=related_alignment)

        if type(results) is tuple:
            clusters, extended = results
            cycles = extended['list_extended_clusters_cycle']
            if 'cycle_paths' in extended:
                cycle_paths = extended['cycle_paths']
            else:
                cycle_paths = None

        else:
            clusters = results
            cycles = None
            cycle_paths = None

        # print "\n", extended

    print ""
    # **************************************************************************************
    # 2. ITERATE THROUGH EACH CLUSTER FOR GENERATING CONTEXTUAL INFORMATION FOR VALIDATION
    # **************************************************************************************
    for cluster_id, cluster in clusters.items():

        count += 1
        nodes = cluster['nodes']
        cluster_size = len(nodes)

        if cycles is None:
            contain_cycle = "no"
        else:
            contain_cycle = 'yes' if cluster_id in cycles else 'no'

        # COMPUTE THE MACHINE EVALUATION
        if cluster_size > 500:
            decision = "NOT COMPUTED [NA]"
        else:
            decision = metric(cluster['links'])["AUTOMATED_DECISION"]

        if size is None or cluster_size <= size:

            # FETCH DATA ABOUT THE PROVIDED RESOURCES
            matrix = investigate_resources(data, resources=nodes)

            if contain_cycle == 'yes':

                final = set()
                if cycle_paths is not None:
                    start_end_paths = cycle_paths[cluster_id]
                    for start_node, end_node, strength in start_end_paths:
                        path_nodes = shortest_paths(cluster["links"], start_node, end_node)
                        for path_list in path_nodes:
                            for node in path_list:
                                final.add(node)

                final = list(final) if len(final) > 0 else None

                # print "NODES IN CYCLE:", final

                count_c += 1

                # WRITE THE FETCHED DATA TO SOURCE
                write_record(count_c, cluster_size, record_format=record_format, matrix=matrix, writer=writer_cycle,
                             cluster_id=cluster_id, machine_decision=decision, separator_size=separator_size,
                             has_cycle=contain_cycle, node_in_cycle=final)

                # ADD A NEW LINE
                writer_cycle.write("\n")

            else:

                count_nc += 1

                # WRITE THE FETCHED DATA TO SOURCE
                write_record(count_nc, cluster_size, record_format=record_format, matrix=matrix, writer=writer_no_cycle,
                             cluster_id=cluster_id, machine_decision=decision, separator_size=separator_size,
                             has_cycle=contain_cycle)

                # ADD A NEW LINE
                writer_no_cycle.write("\n")

            # if count % 10 == 0 or count == 1:
            if count % 10 == 0:
                print "{:6} {:25}{:6}".format(count, cluster_id, decision), \
                    "so far {} has passed".format(datetime.timedelta(seconds=time.time() - start))

        else:
            count_b += 1
            writer_2.write(record_format.format(
                count_b, cluster_id, cluster_size, decision, "", contain_cycle, "", "", ""))

        if count == 5:
            break

    writer_cycle.close()
    writer_no_cycle.close()
    writer_2.close()

    print "RESULT CAN BE FOUND IN {}.".format(directory)
    print "\nJob Done in {}".format(datetime.timedelta(seconds=time.time() - start))


# **************************************************************************************
# GIVEN TWO NODES, FIND THE SHORTEST PATHS
# THE LINK NETWORK INPUT IS A LIST OF TUPLES WHERE A TUPLE REPRESENTS LINKS BETWEEN
# A PAIR OF NODES. THE FUNCTION MAKES USE OF THE NETWORKS LIBRARY
# **************************************************************************************
def shortest_paths(link_network, start_node, end_node):

    print "COMPUTING PATH..."

    # EXTRACT THE NODES FROM THE NETWORK OF LINKS
    nodes = set([n1 for n1, n2 in link_network] + [n2 for n1, n2 in link_network])

    # INSTANTIATE THE GRAPH
    g = nx.Graph()

    # add nodes
    for node in nodes:
        g.add_node(node)

    # add edges
    for edge in link_network:
        g.add_edge(edge[0], edge[1])

    # GET THE LIT OF PATHS
    # results = list(nx.shortest_simple_paths(g, source=start_node, target=end_node))
    results = shortest_paths_lite(g, source=start_node, target=end_node)

    # EXTRACT THE SHORTEST PATH  OF THE SMALLEST SIZE
    # for item in results:
    #
    #     if len(item) == len(results[0]):
    #         final += [item]
    #
    #     else:
    #         break

    print "DONE COMPUTING PATH!"
    return results


def shortest_path_nodes(link_network, start_node, end_node):

    final = set()

    # EXTRACT THE NODES FROM THE NETWORK OF LINKS
    nodes = set([n1 for n1, n2 in link_network] + [n2 for n1, n2 in link_network])

    # INSTANTIATE THE GRAPH
    g = nx.Graph()

    # add nodes
    for node in nodes:
        g.add_node(node)

    # add edges
    for edge in link_network:
        g.add_edge(edge[0], edge[1])

    # GET THE LIT OF PATHS
    results = list(nx.shortest_simple_paths(g, source=start_node, target=end_node))

    # EXTRACT THE SHORTEST PATH
    for item in results:

        if len(item) == len(results[0]):
            for data in item:
                final.add(data)

        else:
            break

    return list(final)


def shortest_paths_lite(g, source, target, weight=None):

    # print g

    # get the shortest but not necessarily unique path
    result = nx.shortest_path(g, source=source, target=target)
    results = []

    # if result is a path, add it to the list and try to find other pathts of same size
    if result is not None:
        results = [result]
        size = len(result)

        # for each of the results of same size found, remove edges to try and find other paths
        for result in results:
            # print result
            partials = []

            # for each pair in the path, remove the link and check the shortest path and add it again
            for i in range(len(result)-1):
                # print "removing ", result[i], ', ', result[i+1]
                g.remove_edge(result[i],result[i+1])
                try:
                    partial = nx.shortest_path(g, source=source, target=target)
                except:
                    partial = []

                # if there is a path of same size, keep it in a set (there can be repetition)
                if len(partial) == size:
                    if partial not in partials:
                        partials += [partial]

                g.add_edge(result[i],result[i+1])

            # add whatever paht found if so
            for p in partials:
                if p not in results:
                    results += [p]
            # print 'new paths: ', partials

    return results


def nodes_in_cycle(specs):

    # specs = {
    #     'investigated_cluster': {"network": links_ve, "start": "Al", 'end': "oladele"},
    #     'extended_cluster': {"network": links_al, "start": "Al", 'end': "oladele"}
    # }

    investigated_cluster = specs["investigated_cluster"]
    extended_cluster = specs["extended_cluster"]
    source = shortest_paths(investigated_cluster["network"], investigated_cluster["start"], investigated_cluster["end"])
    target = shortest_paths(extended_cluster["network"], extended_cluster["start"], extended_cluster["end"])

    nodes = []

    for item in source:
        for element in target:
            nodes += [item + element]

    for item in nodes:
        print item


def evidence_penalty(investigated_diameter, evidence_diameter, penalty_percentage=10):

    penalty = (100 - penalty_percentage * (evidence_diameter - 1)) / float(100)
    return 0 if penalty < 0 else (1 / float(investigated_diameter)) * penalty


def path_factorial(paths):

    size = len(paths)
    for i in range(0, size):
        print paths[i]

        for j in range(i+1, size):
            print "\t", paths[i], paths[j]


# path_factorial(["1", "2", "3", "4"])

def generate_sheet_cyc(data, directory, graph, serialisation_dir, related_alignment=None, separator_size=40, size=None):

    start = time.time()
    count = 0
    count_c = 0
    count_b = 0
    count_nc = 0
    cycles = None
    # extended = None
    header_separator_size = 23

    # FILE DATE
    # date = datetime.date.isoformat(datetime.date.today()).replace('-', '')
    date = (str(datetime.datetime.utcnow())[:-7]).replace('-', '').replace(':', '').replace(' ', '-')
    hashed = Ut.hash_it(graph)
    directory = join(directory, hashed)

    if isdir(directory) is False:
        makedirs(directory)

    # THE WRITER
    writer_cycle = open(join(directory, "EvalSheet_{}_{}_cycle.txt".format(hashed, date)), 'wb')
    writer_no_cycle = open(join(directory, "EvalSheet_{}_{}_noCycle.txt".format(hashed, date)), 'wb')
    writer_2 = open(join(directory, "EvalSheet_{}_{}_biggerThan_{}.txt".format(hashed, date, size)), 'wb')

    # RECORD FORMAT
    record_format = "{{:<7}}{{:<{0}}}{{:<14}}{{:<{0}}}{{:<12}}{{:<12}}{{:<10}}{{:<7}}{{:<{0}}}\n".format(
        header_separator_size)
    print record_format

    # RECORD HEADER
    header = record_format.format(
        "COUNT", "CLUSTER-ID", "CLUSTER-SIZE", "MACHINE-EVAL", "HUMAN-EVAL", "HAS-CYCLE", "NOT GOOD", "CYCLE",
        "RESOURCES")

    # **************************************************************************************
    # 1. GENERATING / EXTRACTING CLUSTERS, EXTENDED CLUSTERS AND LIST OF CLUSTERS IN A CYCLE
    # **************************************************************************************
    # 1.1 IF THE RELATED ALIGNMENT IS NOT PROVIDED, ONLY THE CLUSTERS DICTIONARY IS RETURNED
    if related_alignment is None:
        clusters = links_clustering(graph=graph, serialisation_dir=serialisation_dir)

    # 1.2 IF THE RELATED ALIGNMENT IS PROVIDED, THEN THE EXTENDED CLUSTERS AND CLUSTERS IN A CYCLE
    # IS COMPUTED IF NT SERIALISED OR READ FROM FILE IF SERIALISED
    else:
        clusters, extended = links_clustering(
            graph=graph, serialisation_dir=serialisation_dir, related_linkset=related_alignment)
        cycles = extended['list_extended_clusters_cycle']

        # print "\n", extended

    print ""
    # **************************************************************************************
    # 2. ITERATE THROUGH EACH CLUSTER FOR GENERATING CONTEXTUAL INFORMATION FOR VALIDATION
    # **************************************************************************************
    for cluster_id, cluster in clusters.items():

        count += 1
        nodes = cluster['nodes']
        cluster_size = len(nodes)

        if cycles is None:
            contain_cycle = "no"
        else:
            contain_cycle = 'yes' if cluster_id in cycles else 'no'

        # COMPUTE THE MACHINE EVALUATION
        if cluster_size > 500:
            decision = "NOT COMPUTED [NA]"
        else:
            decision = metric(cluster['links'])["AUTOMATED_DECISION"]

        if size is None or cluster_size <= size:

            # FETCH DATA ABOUT THE PROVIDED RESOURCES
            matrix = investigate_resources(data, resources=nodes)

            if contain_cycle == 'yes':

                count_c += 1
                # WRITING THE FILE HEADER
                writer_cycle.write(header)

                # WRITE THE FETCHED DATA TO SOURCE
                write_record(count_c, cluster_size, record_format=record_format, matrix=matrix, writer=writer_cycle,
                             cluster_id=cluster_id, machine_decision=decision, separator_size=separator_size,
                             has_cycle=contain_cycle)

                # ADD A NEW LINE
                writer_cycle.write("\n")

                if count_c == 150:
                    break

            else:

                count_nc += 1
                # WRITING THE FILE HEADER
                writer_no_cycle.write(header)

                # WRITE THE FETCHED DATA TO SOURCE
                write_record(count_nc, cluster_size, record_format=record_format, matrix=matrix, writer=writer_no_cycle,
                             cluster_id=cluster_id, machine_decision=decision, separator_size=separator_size,
                             has_cycle=contain_cycle)

                # ADD A NEW LINE
                writer_no_cycle.write("\n")

            # if count % 10 == 0 or count == 1:
            if count % 10 == 0:
                print "{:6} {:25}{:6}".format(count, cluster_id, decision), \
                    "so far {} has passed".format(datetime.timedelta(seconds=time.time() - start))

        else:
            count_b += 1
            # WRITING THE FILE HEADER
            writer_2.write(header)
            writer_2.write(
                record_format.format(count_b, cluster_id, cluster_size, decision, "", contain_cycle, "", "", ""))

        # if count == 50:
        #     break

    writer_cycle.close()
    writer_no_cycle.close()
    writer_2.close()

    print "\nJob Done in {}".format(datetime.timedelta(seconds=time.time() - start))


# def process_cluster(
#         data, resources, network, writer, with_header, machine_decision, separator_size=20, cluster_id=""):
#
#     # RECORD ITEM SEPARATOR
#     # separator_size = 20
#
#     # FILE DATE
#     # date = datetime.date.isoformat(datetime.date.today()).replace('-', '')
#
#     # THE WRITER
#     # writer = open(join(directory, "EvalSheet_{}.txt".format(date)), 'wb')
#
#     # RECORD FORMAT
#     record_format = "{{:{0}}}{{:<{0}}}{{:<{0}}}{{:<{0}}}{{:<{0}}}{{:<{0}}}{{:<{0}}}\n".format(separator_size)
#     # print record_format
#
#     # THE HEADER OF THE FILE
#     # header = record_format.format(
#     #     "CLUSTER-ID", "CLUSTER-SIZE", "MACHINE-EVAL", "HUMAN-EVAL", "NOT GOOD", "CYCLE", "RESOURCES")
#     # writer.write(header)
#
#     # FETCH DATA ABOUT THE PROVIDED RESOURCES
#     matrix = investigate_resources(data, resources)
#
#     # decision = metric(network)["AUTOMATED_DECISION"]
#
#     # WRITE THE FETCHED DATA TO SOURCE
#     write_record(record_format=record_format, matrix=matrix, writer=writer,
#                  cluster_id=cluster_id, machine_decision=machine_decision)
#     writer.write("\n")
#
#     # END OF FILE
#     # writer.close()
